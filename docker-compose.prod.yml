# ============================================
# DOCKER COMPOSE - MODE PRODUCTION OPTIMISÉ
# ============================================
# Usage: pnpm docker:prod:build
# Optimisé pour production: sécurité, performance, cache
# ============================================

services:
  # ==========================================
  # SERVICE 1 : POSTGRESQL DATABASE
  # ==========================================
  database:
    image: postgres:16-alpine
    container_name: museum-db-prod
    restart: always
    environment:
      POSTGRES_DB: museumvoice
      POSTGRES_USER: museum_admin
      POSTGRES_PASSWORD: ${DB_PASSWORD:-Museum@2026!Secure}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=fr_FR.UTF-8"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data_prod:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U museum_admin -d museumvoice"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - museum-network
    deploy:
      resources:
        reservations:
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ==========================================
  # SERVICE 2 : OLLAMA LLM LOCAL
  # ==========================================
  ollama:
    image: ollama/ollama:latest
    container_name: museum-ollama-prod
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama_data_prod:/root/.ollama
      - ./scripts/ollama-entrypoint.sh:/usr/local/bin/ollama-entrypoint.sh:ro
    networks:
      - museum-network
    entrypoint: ["/bin/bash", "/usr/local/bin/ollama-entrypoint.sh"]
    healthcheck:
      test: ["CMD-SHELL", "ollama list > /dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        reservations:
          memory: 8G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ==========================================
  # SERVICE 3 : BACKEND PYTHON FLASK + RAG
  # ==========================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
      cache_from:
        - museum-backend-prod:latest
      args:
        BUILDKIT_INLINE_CACHE: 1
    image: museum-backend-prod:latest
    container_name: museum-backend-prod
    restart: always
    depends_on:
      database:
        condition: service_healthy
      ollama:
        condition: service_healthy
    environment:
      # Database PostgreSQL
      DB_HOST: database
      DB_PORT: 5432
      DB_NAME: museumvoice
      DB_USER: museum_admin
      DB_PASSWORD: ${DB_PASSWORD:-Museum@2026!Secure}
      
      # Flask PRODUCTION
      FLASK_ENV: production
      FLASK_APP: rag.main_postgres
      PYTHONUNBUFFERED: 1
      
      # Ollama LOCAL
      OLLAMA_API_URL: http://ollama:11434
      OLLAMA_MODEL: ministral-3:3b
      
      # Gunicorn Performance (auto-detect CPU)
      GUNICORN_WORKERS: ${GUNICORN_WORKERS:-auto}
      GUNICORN_THREADS: ${GUNICORN_THREADS:-4}
      GUNICORN_TIMEOUT: ${GUNICORN_TIMEOUT:-180}
      GUNICORN_WORKER_CONNECTIONS: ${GUNICORN_WORKER_CONNECTIONS:-1000}
      
      # LLM API Keys (fallback)
      GROQ_API_KEY: ${GROQ_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
    ports:
      - "5000:5000"
    volumes:
      - uploads_data_prod:/app/uploads
    networks:
      - museum-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        reservations:
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ==========================================
  # SERVICE 4 : NEXT.JS EDITOR/DASHBOARD
  # ==========================================
  app:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
      cache_from:
        - museum-app-prod:latest
      args:
        BUILDKIT_INLINE_CACHE: 1
        NEXT_TELEMETRY_DISABLED: 1
        # ⚠️ Variables NEXT_PUBLIC_* doivent être passées au build
        NEXT_PUBLIC_BACKEND_URL: ${NEXT_PUBLIC_BACKEND_URL:-http://51.38.188.211:5000}
        NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://51.38.188.211:3000}
    image: museum-app-prod:latest
    container_name: museum-app-prod
    restart: always
    depends_on:
      database:
        condition: service_healthy
      backend:
        condition: service_healthy
    environment:
      # Database
      DB_TYPE: postgresql
      DB_HOST: database
      DB_PORT: 5432
      DB_NAME: museumvoice
      DB_USER: museum_admin
      DB_PASSWORD: ${DB_PASSWORD:-Museum@2026!Secure}
      
      # Backend API (internal Docker network)
      BACKEND_API_URL: http://backend:5000
      BACKEND_URL: http://backend:5000
      
      # Admin API (internal Docker network - for internal container communication)
      ADMIN_API_URL: http://app:3000
      
      # Public URLs (for browser-side requests - accessible from outside)
      # ⚠️ Override in .env.prod with your domain or keep VPS IP
      NEXT_PUBLIC_BACKEND_URL: ${NEXT_PUBLIC_BACKEND_URL:-http://51.38.188.211:5000}
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://51.38.188.211:3000}
      NEXT_PUBLIC_CLIENT_URL: ${NEXT_PUBLIC_CLIENT_URL:-http://51.38.188.211:8080}
      
      # Next.js PRODUCTION
      NODE_ENV: production
      NEXT_TELEMETRY_DISABLED: 1
      
      # App
      APP_ENV: production
    ports:
      - "3000:3000"
    volumes:
      - uploads_data_prod:/app/public/uploads
    networks:
      - museum-network
    healthcheck:
      test: ["CMD-SHELL", "node -e \"require('http').get('http://localhost:3000', (res) => process.exit(res.statusCode === 200 ? 0 : 1))\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        reservations:
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ==========================================
  # SERVICE 5 : REACT FRONTEND CLIENT (NGINX)
  # ==========================================
  client:
    build:
      context: ./client-frontend
      dockerfile: Dockerfile
      cache_from:
        - museum-client-prod:latest
      args:
        BUILDKIT_INLINE_CACHE: 1
        # ⚠️ Build args - URLs publiques accessibles depuis le navigateur
        REACT_APP_BACKEND_URL: ${REACT_APP_BACKEND_URL:-http://51.38.188.211:5000}
        REACT_APP_ADMIN_URL: ${REACT_APP_ADMIN_URL:-http://51.38.188.211:3000}
    image: museum-client-prod:latest
    container_name: museum-client-prod
    restart: always
    depends_on:
      backend:
        condition: service_healthy
    environment:
      # Ces variables sont aussi passées au runtime (mais le build est prioritaire)
      REACT_APP_BACKEND_URL: ${REACT_APP_BACKEND_URL:-http://51.38.188.211:5000}
      REACT_APP_ADMIN_URL: ${REACT_APP_ADMIN_URL:-http://51.38.188.211:3000}
    ports:
      - "8080:80"
    volumes:
      - uploads_data_prod:/usr/share/nginx/html/uploads:ro
    networks:
      - museum-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        reservations:
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

# ==========================================
# VOLUMES PERSISTANTS PROD (Named volumes)
# ==========================================
volumes:
  postgres_data_prod:
    name: museum-postgres-data-prod
    driver: local
  uploads_data_prod:
    name: museum-uploads-data-prod
    driver: local
  ollama_data_prod:
    name: museum-ollama-models-prod
    driver: local

# ==========================================
# RÉSEAU INTERNE
# ==========================================
networks:
  museum-network:
    name: museum-network-prod
    driver: bridge

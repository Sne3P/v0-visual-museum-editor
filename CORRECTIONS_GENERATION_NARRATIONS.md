# ğŸ”§ CORRECTIONS SYSTÃˆME DE GÃ‰NÃ‰RATION DE NARRATIONS

## âŒ PROBLÃˆMES IDENTIFIÃ‰S

### 1. **Chunks Inutiles CrÃ©Ã©s par le Frontend**
**Fichier**: `core/services/database.service.ts` ligne 280-283  
**ProblÃ¨me**: CrÃ©ation de chunks vides avec juste `artwork.name`  
**Impact**: Pollue la base de donnÃ©es, aucun contenu RAG rÃ©el

### 2. **PDF Jamais Extrait pour RAG**
**ProblÃ¨me**: Le PDF est uploadÃ© mais jamais traitÃ© pour extraire le texte  
**Impact**: Pas de chunks RAG â†’ pas de contexte â†’ narrations inventÃ©es

### 3. **Prompts Peu Factuels**
**Fichier**: `backend/rag/core/ollama_generator.py`  
**ProblÃ¨mes**:
- TempÃ©rature trop haute (0.4) â†’ crÃ©ativitÃ© excessive
- Validation trop permissive (ligne 291-295)
- Instructions peu claires sur interdiction d'inventer

### 4. **Configuration Ollama Non OptimisÃ©e**
**ProblÃ¨me**: ParamÃ¨tres par dÃ©faut, pas de force CPU explicite  
**Impact**: Peut utiliser GPU, performances variables

---

## âœ… CORRECTIONS APPLIQUÃ‰ES

### 1. **Suppression Chunks Frontend** âœ…
**Fichiers modifiÃ©s**:
- `core/services/database.service.ts` (lignes 56-76, 100-118, 275-283, 482)
- `app/api/save-to-db/route.ts` (lignes 191-199)

**Actions**:
- âŒ SupprimÃ© crÃ©ation `chunks.push({ chunk_text: artwork.name })`
- âŒ SupprimÃ© `chunkIdCounter` inutile
- âŒ SupprimÃ© interface `chunks: Array<...>`
- âœ… AjoutÃ© commentaire: "Chunks crÃ©Ã©s par backend lors extraction PDF"

### 2. **GÃ©nÃ©rateur Factuel OptimisÃ©** âœ…
**Nouveau fichier**: `backend/rag/core/ollama_generator_improved.py`

**AmÃ©liorations**:
- âœ… **TempÃ©rature ultra-basse**: 0.2 (vs 0.4) â†’ factuel strict
- âœ… **top_p rÃ©duit**: 0.75 (vs 0.9) â†’ moins de crÃ©ativitÃ©
- âœ… **top_k rÃ©duit**: 40 (vs 50) â†’ choix plus dÃ©terministes
- âœ… **Force CPU**: `num_gpu: 0` â†’ utilise CPU/RAM uniquement
- âœ… **Prompts clairs**: Instructions explicites "N'invente RIEN"
- âœ… **Validation stricte**: DÃ©tecte spÃ©culation, salutations, formules

**RÃ¨gles linguistiques implÃ©mentÃ©es**:
- âŒ Pas de pluriel ("les amis" interdit)
- âŒ Pas de genre sauf si factuel
- âŒ Pas de salutations ("Bonjour", "Salut", "Aujourd'hui")
- âŒ Pas de formules d'accroche ("Voici", "Regardez")
- âœ… Commence DIRECTEMENT par le contenu

**Validation anti-hallucination**:
```python
# DÃ©tecte et REJETTE:
- "on raconte", "la lÃ©gende", "selon certains"
- "probablement", "peut-Ãªtre", "il se pourrait"
- Salutations en dÃ©but
- Longueur anormale (< 100 ou > 350 mots)
```

### 3. **Configuration Ollama OptimisÃ©e**
```python
{
    "temperature": 0.2,        # Ultra-factuel (vs 0.4)
    "top_p": 0.75,            # Strict (vs 0.9)
    "top_k": 40,              # RÃ©duit (vs 50)
    "num_predict": 200,       # 200 mots max
    "num_ctx": 2048,          # Contexte rÃ©duit = rapide
    "num_batch": 1024,        # Batch CPU
    "num_thread": 8,          # 8 threads CPU
    "num_gpu": 0,             # FORCE CPU (pas GPU)
    "repeat_penalty": 1.2     # Anti-rÃ©pÃ©tition forte
}
```

---

## ğŸ”„ FLUX CORRECT Ã€ IMPLÃ‰MENTER

### Pipeline Complet:
```
1. Upload PDF via frontend
   â†“
2. /api/extract-pdf-metadata
   - Extrait texte PDF (PyPDF2)
   - Parse sections (modÃ¨le structurÃ©)
   - Sauvegarde mÃ©tadonnÃ©es en BDD
   â†“
3. Backend crÃ©e chunks RAG
   - chunk_creator_postgres.py
   - DÃ©coupe en chunks sÃ©mantiques
   - Sauvegarde chunks en BDD
   â†“
4. Backend crÃ©e embeddings
   - SentenceTransformer
   - Calcule vecteurs pour chaque chunk
   - Sauvegarde dans table `embeddings`
   â†“
5. Backend crÃ©e index FAISS
   - Construit index vectoriel
   - Sauvegarde sur disque
   â†“
6. PrÃ©gÃ©nÃ©ration narrations
   - Pour chaque profil (36 combinaisons)
   - RAG: rÃ©cupÃ¨re chunks pertinents
   - Ollama: gÃ©nÃ¨re narration FACTUELLE
   - Validation stricte
   - Sauvegarde dans `pregenerations`
```

---

## ğŸ“‹ TÃ‚CHES RESTANTES

### PrioritÃ© 1: Activer Nouveau GÃ©nÃ©rateur
- [ ] Remplacer import dans `ollama_pregeneration_complete.py`:
  ```python
  # Ancien
  from rag.core.ollama_generator import get_ollama_generator
  
  # Nouveau
  from rag.core.ollama_generator_improved import get_factual_generator as get_ollama_generator
  ```

### PrioritÃ© 2: Dashboard PrÃ©gÃ©nÃ©ration
- [ ] Ajouter bouton "GÃ©nÃ©rer Narrations" dans `/admin/dashboard`
- [ ] Appelle `/api/backend/pregenerate` avec `oeuvre_id`
- [ ] Affiche progression (36 narrations)
- [ ] Affiche rÃ©sultat (gÃ©nÃ©rÃ©es/erreurs)

### PrioritÃ© 3: API PrÃ©gÃ©nÃ©ration
- [ ] CrÃ©er `/app/api/backend/pregenerate/route.ts`:
  ```typescript
  POST /api/backend/pregenerate
  Body: { oeuvre_id: number, force_regenerate?: boolean }
  
  Appelle backend Python:
  - CrÃ©e chunks (si pas dÃ©jÃ  fait)
  - CrÃ©e embeddings
  - Construit FAISS
  - GÃ©nÃ¨re 36 narrations
  
  Returns: { success, stats: { generated, updated, errors } }
  ```

### PrioritÃ© 4: Tests
- [ ] Upload PDF test
- [ ] VÃ©rifier chunks crÃ©Ã©s (`SELECT * FROM chunk WHERE oeuvre_id=X`)
- [ ] VÃ©rifier embeddings (`SELECT COUNT(*) FROM embeddings e JOIN chunk c ...`)
- [ ] Lancer prÃ©gÃ©nÃ©ration
- [ ] VÃ©rifier narrations (`SELECT * FROM pregenerations WHERE oeuvre_id=X`)
- [ ] Tester gÃ©nÃ©ration parcours

---

## ğŸ” VÃ‰RIFICATIONS

### Chunks en BDD:
```sql
-- Doit retourner 5-8 chunks par Å“uvre (pas 1 seul avec le nom)
SELECT oeuvre_id, COUNT(*) as chunk_count, 
       AVG(LENGTH(chunk_text)) as avg_length
FROM chunk
GROUP BY oeuvre_id;
```

### Embeddings:
```sql
-- Doit avoir autant d'embeddings que de chunks
SELECT 
  (SELECT COUNT(*) FROM chunk) as total_chunks,
  (SELECT COUNT(*) FROM embeddings) as total_embeddings;
```

### Index FAISS:
```bash
# Doit exister sur disque
ls -la /app/rag/indexes/museum_postgres/artwork_*.faiss
ls -la /app/rag/indexes/museum_postgres/artwork_*.mapping
```

### Narrations Factuelles:
```sql
-- VÃ©rifier contenu (pas de "Bonjour", "peut-Ãªtre", etc.)
SELECT pregeneration_text 
FROM pregenerations 
WHERE oeuvre_id = 1 
LIMIT 5;
```

---

## ğŸ“Š MÃ‰TRIQUES CIBLES

- **Chunks par Å“uvre**: 5-8 (vs 1 actuellement)
- **Longueur chunk**: 200-500 caractÃ¨res (vs 20 actuellement)
- **Temps gÃ©nÃ©ration**: ~3-5s par narration (36 narrations = ~2min)
- **Taux validation**: > 90% (rejection stricte hallucinations)
- **Contenu factuel**: 100% basÃ© sur PDF (zÃ©ro invention)

---

## ğŸš€ POUR ACTIVER

1. **Backend**: RedÃ©marrer avec nouveau gÃ©nÃ©rateur
   ```bash
   docker-compose restart backend
   ```

2. **Test Upload PDF**:
   - Uploader un PDF via `/editor`
   - VÃ©rifier extraction: `SELECT * FROM oeuvres WHERE oeuvre_id=X`
   - Chunks auto-crÃ©Ã©s? `SELECT COUNT(*) FROM chunk WHERE oeuvre_id=X`

3. **Si chunks pas crÃ©Ã©s**:
   - Appeler manuellement: `POST /api/backend/create-chunks` avec `{oeuvre_id}`
   - Ou intÃ©grer dans `extract-pdf-metadata`

4. **GÃ©nÃ©rer Narrations**:
   - `POST /api/backend/pregenerate` avec `{oeuvre_id}`
   - Attend ~2min pour 36 narrations
   - VÃ©rifier: `SELECT COUNT(*) FROM pregenerations WHERE oeuvre_id=X` â†’ doit Ãªtre 36

---

## âš ï¸ POINTS D'ATTENTION

- **Ne jamais truncate chunk** sans vÃ©rifier pregenerations
- **Ollama doit tourner** avant prÃ©gÃ©nÃ©ration
- **PDF doit Ãªtre valide** et structurÃ©
- **RAM**: Mistral utilise ~4-8 GB avec num_ctx=2048
- **CPU**: Avec 8 threads, occupe ~60-80% pendant gÃ©nÃ©ration

---

## ğŸ“ DOCUMENTATION AJOUTÃ‰E

- `ollama_generator_improved.py`: Commentaires dÃ©taillÃ©s
- `database.service.ts`: Explication suppression chunks
- `save-to-db/route.ts`: RÃ©fÃ©rence pipeline RAG
- Ce fichier: Guide complet corrections

"""
API Flask pour Museum Voice Backend





































































































































































































































































































































































































**Prochaine √©tape** : Impl√©menter optimisation `num_threads` et mesurer impact r√©el---**Gain total estim√©** : **Production 6x plus rapide** avec optimisations CPU3. **Nettoyage dependencies** (image Docker plus l√©g√®re)2. **Parall√©lisation g√©n√©ration** (gain 3x suppl√©mentaire)1. **num_threads ‚Üí cpu_count()** (gain 2-4x)**Am√©liorations prioritaires** :- ‚úÖ Code legacy nettoy√©- ‚úÖ Syst√®me dynamique et flexible- ‚úÖ M√©tadonn√©es compl√®tes utilis√©es- ‚úÖ Pas de RAG complexe inutile**Le backend est d√©j√† bien optimis√©** :## ‚úÖ CONCLUSION---3. Valider longueur narrations (~120-160 mots/min)2. Tester avec diff√©rents profils (enfant vs senior)1. V√©rifier que les narrations g√©n√©r√©es utilisent bien les AI indications### **Validation qualit√©** :3. Monitorer RAM/CPU pendant g√©n√©ration2. Tester parall√©lisation avec 2 workers1. Mesurer temps g√©n√©ration 1 ≈ìuvre (36 combos) AVANT/APR√àS### **Tests de performance** :   ```   - faiss-cpu>=1.7.0   - sentence-transformers>=2.2.0   # requirements.txt   ```bash2. **Supprimer d√©pendances inutiles**   ```   "num_threads": min(multiprocessing.cpu_count(), 16)   import multiprocessing   ```python1. **Modifier `num_threads` dans ollama_generation.py**### **Changements imm√©diats** (Quick Wins) :## üéØ RECOMMANDATIONS FINALES---**Action recommand√©e** : Supprimer de `requirements.txt` pour r√©duire l'image Docker```‚ùå faiss-cpu>=1.7.0              # RAG legacy (FAISS)‚ùå sentence-transformers>=2.2.0  # RAG legacy (embeddings)```txt### **requirements.txt - Packages inutilis√©s** :## üì¶ D√âPENDANCES √Ä NETTOYER---- [ ] Load balancing multi-instances Ollama- [ ] Rate limiting Ollama- [ ] Queue system (Celery/RQ) pour g√©n√©ration batch### **4. Scalabilit√© (Priorit√© BASSE)**- [ ] Fine-tuning prompts selon feedback utilisateurs- [ ] A/B testing temp√©rature (0.5 vs 0.7)- [ ] Tester autres mod√®les (gemma3:8b, llama3.2)### **3. Qualit√© LLM (Priorit√© BASSE)**- [ ] M√©triques Prometheus/Grafana- [ ] Alertes si g√©n√©ration > 10s- [ ] Logger temps g√©n√©ration par combo### **2. Monitoring (Priorit√© MOYENNE)**- [ ] Mesurer performance r√©elle sur serveur production- [ ] Tester parall√©lisation g√©n√©ration (ThreadPoolExecutor)- [ ] Augmenter `num_threads` √† `cpu_count()`### **1. Performance (Priorit√© HAUTE)**## üîß POINTS √Ä AM√âLIORER---7. ‚úÖ **Endpoints propres** : Legacy RAG supprim√©6. ‚úÖ **DB flexible** : JSONB + relations N:N5. ‚úÖ **Cache intelligent** : Pas de r√©g√©n√©ration inutile4. ‚úÖ **M√©tadonn√©es compl√®tes** : TOUS les champs utilis√©s3. ‚úÖ **Prompts optimis√©s** : Instructions claires pour LLM2. ‚úÖ **Syst√®me dynamique** : N crit√®res support√©s (DB-driven)1. ‚úÖ **Architecture simple** : Pas de complexit√© RAG inutile## ‚úÖ POINTS FORTS DU SYST√àME ACTUEL---**Impact global estim√©** : **Production 6x plus rapide**| **Temps 10 ≈ìuvres** | ~30 min | ~5 min | **6x** || **Temps par ≈ìuvre (36 combos)** | ~3 min | ~30 sec | **6x** || **G√©n√©ration s√©quentielle** | 1 combo/fois | 3 combos/fois | 3x || **Threads Ollama** | 4 | 8-16 | 2-4x ||----------|--------|----------|------|| M√©trique | Actuel | Optimis√© | Gain |## üìä PERFORMANCE ACTUELLE vs OPTIMIS√âE---**Priorit√©** : **BASSE** (performance d√©j√† acceptable)```# Possible optimisation : extraction par page parall√®le# V√©rifier si pdfplumber utilis√© efficacement```python**Recommandations** :**Performance actuelle** : ~3-5 secondes par PDF (extraction m√©tadonn√©es)### **5. Extraction PDF (model_pdf_processor.py)**---**‚úÖ D√âJ√Ä IMPL√âMENT√â** - Pas de g√©n√©ration inutile```    continue    stats['skipped'] += 1if existing:existing = self._check_existing(oeuvre_id, combinaison)# V√©rifie si existe d√©j√† AVANT de g√©n√©rer```python**√âtat actuel** :### **4. Cache pr√©g√©n√©rations (Optimisation DB)**---**Recommandation** : **OK** (timeout raisonnable pour LLM)```timeout_s: int = 15000  # 15 secondes (ligne 27)```python**√âtat actuel** :### **3. Timeout Ollama**---- Tester avec 2-3 workers maximum d'abord- V√©rifier limites Ollama (m√©moire GPU)**‚ö†Ô∏è Attention** : **Impact estim√©** : **3x plus rapide** (~1 minute par ≈ìuvre)```    results = [f.result() for f in futures]    futures = [executor.submit(generate_mediation, combo) for combo in combinaisons]with ThreadPoolExecutor(max_workers=3) as executor:from concurrent.futures import ThreadPoolExecutor# Parall√©lisation avec ThreadPoolExecutor```python**Optimisation possible** :- Temps total : 36 √ó 5s = **~3 minutes par ≈ìuvre**- G√©n√®re 36 narrations **s√©quentiellement** (une apr√®s l'autre)**√âtat actuel** : ### **2. Performance g√©n√©ration s√©quentielle**---- Serveur 16 c≈ìurs : **4x plus rapide** (4‚Üí16 threads)- Serveur 8 c≈ìurs : **2x plus rapide** (4‚Üí8 threads)**Impact estim√©** : ```"num_threads": min(multiprocessing.cpu_count(), 16)  # Utilise tous les c≈ìurs (max 16)import multiprocessing```python**Solution** :```"num_threads": 4  # ‚ö†Ô∏è FIXE - sous-utilise le serveur# ollama_generation.py ligne 145```python**Probl√®me actuel** :### **1. Performance CPU (Critique)**## üöÄ OPTIMISATIONS RECOMMAND√âES---**Raison** : Syst√®me RAG/embeddings/FAISS plus utilis√© ‚Üí g√©n√©ration directe depuis DB```‚ùå GET /api/chunks/<oeuvre_id>‚ùå POST /api/chunks/create/<oeuvre_id>‚ùå POST /api/rag/search‚ùå POST /api/rag/faiss/build-global‚ùå POST /api/rag/faiss/build/<oeuvre_id>‚ùå POST /api/rag/embeddings/create/<oeuvre_id># main_postgres.py```python### **Endpoints API supprim√©s** :---**Total nettoy√©** : ~1150 lignes de code legacy + indexes FAISS```‚úÖ backend/rag/core/ollama_pregeneration_complete.py # 506 lignes (ancien syst√®me avec RAG)‚úÖ backend/rag/core/rag_engine_postgres.py           # 443 lignes‚úÖ backend/rag/traitement/chunk_creator_postgres.py  # 193 lignes‚úÖ backend/rag/indexes/                        # Dossier complet FAISS```bash### **Fichiers nettoy√©s** :## üóëÔ∏è FICHIERS SUPPRIM√âS (Legacy RAG System)---**‚úÖ R√âSULTAT** : **TOUTES les m√©tadonn√©es utilis√©es** (sauf champs vides)| `contexte_commande` | ‚úÖ | 7000 chars || `iconographie_symbolique` | ‚úÖ | 7000 chars || `analyse_materielle_technique` | ‚úÖ | 7000 chars || `description` | ‚úÖ | 7000 chars || `provenance` | ‚úÖ | Non || `materiaux_technique` | ‚úÖ | Non || `room` | ‚úÖ | Non || `date_oeuvre` | ‚úÖ | Non || `artist` | ‚úÖ | Non || `title` | ‚úÖ | Non ||-------|---------|---------|| Champ | Utilis√© | Tronqu√© | ### **Toutes les m√©tadonn√©es ≈ìuvre utilis√©es ?**---**‚úÖ R√âSULTAT** : **TOUS les champs crit√®res sont utilis√©s correctement**| `is_required` | ‚úÖ | Validation frontend/backend || `is_active` | ‚úÖ | Filtre lors du chargement || `ordre` | ‚úÖ | Tri affichage frontend || `ai_indication` | ‚úÖ | Prompt (instruction LLM) || `description` | ‚úÖ | Prompt "(D√©finition : ...)" || `name` | ‚úÖ | Prompt "CONTRAINTE : AGE : Enfant" || `criteria_id` | ‚úÖ | Sauvegarde DB ||-------|---------|-----|| Champ | Utilis√© | O√π |### **Tous les champs utilis√©s ?**## ‚úÖ UTILISATION DES CRIT√àRES - V√âRIFICATION COMPL√àTE---- Pas de duplication (UPDATE si existe d√©j√†)- Recherche optimis√©e via JSONB + tables relations- Syst√®me DYNAMIQUE : supporte N crit√®res (pas limit√© √† 3)**‚úÖ CONFIRMATION** :```  ‚Üí Relations N:N pour recherche rapide+ INSERT INTO pregeneration_criterias (pregeneration_id, criteria_id))  pregeneration_text  criteria_combination,  # JSONB : {"age": 1, "thematique": 4, "style_texte": 7}  oeuvre_id, INSERT INTO pregenerations (# pregeneration_db.py : add_pregeneration()```python### **6. SAUVEGARDE DB (JSONB flexible)**---- Recommandation : `num_threads = os.cpu_count()` (8-16 threads sur serveur moderne)- `num_threads: 4` ‚Üí Devrait utiliser **tous les c≈ìurs CPU disponibles****‚ö†Ô∏è POINT D'OPTIMISATION** :```}  }    "num_threads": 4    # ‚ö†Ô∏è √Ä OPTIMISER    "num_predict": -1,  # Illimit√©    "temperature": 0.5,  "options": {  "messages": [system, user],  "model": "ministral-3:3b",{POST http://localhost:11434/api/chat# ollama_chat()```python### **5. APPEL OLLAMA API**---- Prompt optimis√© pour g√©n√©ration orale (audioguide)- **AI indications** utilis√©es pour ajuster le vocabulaire- **Descriptions des crit√®res** int√©gr√©es au prompt**‚úÖ CONFIRMATION** :```  - R√àGLES D'√âCRITURE : Oralit√©, guidage visuel, v√©racit√©, structure  - SOURCE UNIQUE : M√©tadonn√©es ≈ìuvre (reformuler, pas copier)  - INSTRUCTIONS PERSONNALISATION : {age, th√®me, style} avec descriptions + AI indications  - PARAM√àTRES : Langue, dur√©e cibleUSER:SYSTEM: "Tu es un guide expert, improvise des visites captivantes..."# build_single_work_mediation_prompt()```python### **4. PROMPT SYSTEM + USER (LLM Instructions)**---- Troncature intelligente (7000 chars max par champ)- Texte structur√© et format√© pour LLM- **TOUS** les champs m√©tadonn√©es utilis√©s**‚úÖ CONFIRMATION** :```- contexte_commande- iconographie_symbolique- analyse_materielle_technique- description (7000 chars max)- title, artist, date_oeuvre, room, technique, provenance# R√©cup√®re TOUS les champs de l'≈ìuvre :# ollama_generation.py : oeuvre_to_prompt_text()```python### **3. CONSTRUCTION DU PROMPT (Artwork ‚Üí Texte)**---- Chaque combo contient : `{id, name, description, ai_indication}`- **TOUTES** les combinaisons g√©n√©r√©es (pas de filtrage)**‚úÖ CONFIRMATION** : ```‚Üí 36 combinaisons (4 ages √ó 3 th√®mes √ó 3 styles)combinations = itertools.product(*criteres_dict.values())# ollama_generation.py : generate_combinaisons()```python### **2. G√âN√âRATION DES COMBINAISONS**---- AI indications int√©gr√©es au prompt- Descriptions compl√®tes utilis√©es- Tous les crit√®res sont lus depuis la base `criterias`**‚úÖ CONFIRMATION** : ```  }    'style_texte': [{id: 7, name: 'Analyse', ...}, ...]    'thematique': [{id: 4, name: 'Technique picturale', ...}, ...],    'age': [{id: 1, name: 'Enfant', description: '...', ai_indication: '...'}, ...],‚Üí { all_criteres = get_criteres()  # R√©cup√®re tous les types + options depuis DB# core/criteria_service.py```python### **1. CHARGEMENT DES CRIT√àRES (DB dynamique)**## üéØ GRANDES √âTAPES DE G√âN√âRATION---```‚Üí save to pregenerations (JSONB)‚Üí Ollama API (ministral-3:3b) ‚Üí‚Üí build_prompt() avec m√©tadonn√©es compl√®tes ‚ÜíDB criterias ‚Üí generate_combinaisons() ‚Üí ```**Flux simplifi√©** :**M√©thode** : **DIRECTE** - Pas de RAG, pas d'embeddings, pas de FAISS  **G√©n√©rateur utilis√©** : `ollama_generation.py` (454 lignes)  ### ‚úÖ SYST√àME ACTUEL (Production)## üìã R√âSUM√â EX√âCUTIF---**√âtat**: ‚úÖ **BACKEND OPTIMIS√â ET NETTOY√â****Date**: 2024-01-15  Utilise PostgreSQL Docker
"""

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import sys
import os
from pathlib import Path
import psycopg2
import psycopg2.extras
import requests

from .core.ollama_generation import OllamaMediationSystem

# Import modules PostgreSQL (relatifs depuis rag/)
from .core.db_postgres import (
    init_postgres_db, get_artwork, get_all_artworks,
    search_artworks, add_artwork, add_artist, add_movement,
    get_artwork_sections, get_artwork_anecdotes,
    add_section, add_anecdote,
    _connect_postgres, get_criteres
)

from .core.pregeneration_db import (
    add_pregeneration, get_pregeneration,
    get_artwork_pregenerations, get_pregeneration_stats
)

# Import du processeur PDF
from .model_pdf_processor import ModelCompliantPDFProcessor

# Import des routes TTS
from .tts.routes import tts_bp

app = Flask(__name__)
CORS(app)  # Permettre requ√™tes depuis Next.js

# Enregistrer les blueprints
app.register_blueprint(tts_bp)

# Initialiser PostgreSQL au d√©marrage
print("üîÑ Initialisation PostgreSQL...")
try:
    init_postgres_db()
    print("‚úÖ PostgreSQL pr√™t")
except Exception as e:
    print(f"‚ö†Ô∏è Erreur PostgreSQL: {e}")


# ===== HEALTHCHECK =====

@app.route('/health', methods=['GET'])
def health():
    """Healthcheck pour Docker"""
    return jsonify({
        'status': 'healthy',
        'service': 'museum-backend',
        'database': 'postgresql'
    })


# ===== SERVEUR DE FICHIERS STATIQUES =====

@app.route('/uploads/<path:filepath>')
def serve_uploads(filepath):
    """
    Sert les fichiers upload√©s (audio, PDF)
    Exemple: /uploads/audio/parcours_8/oeuvre_1.wav
    """
    try:
        upload_dir = '/app/uploads'
        return send_from_directory(upload_dir, filepath)
    except FileNotFoundError:
        return jsonify({'error': 'File not found'}), 404


# ===== API CRIT√àRES DYNAMIQUES =====

@app.route('/api/criteria-types', methods=['GET'])
def get_criteria_types_legacy():
    """
    Compat client React: retourne les types de crit√®res
    Format attendu: { success: true, types: [{ type_name, label, ordre, is_required }] }
    """
    try:
        from .core.criteria_service import criteria_service

        types = criteria_service.get_criteria_types()

        # Adapter le format pour le frontend client (type_name au lieu de type)
        adapted = [
            {
                'type_name': t['type'],
                'label': t['label'],
                'ordre': t['ordre'],
                'is_required': t['is_required']
            }
            for t in types
        ]

        return jsonify({
            'success': True,
            'types': adapted
        })
    except Exception as e:
        return jsonify({ 'success': False, 'error': str(e) }), 500


@app.route('/api/criterias', methods=['GET'])
def get_criterias_query():
    """
    Compat client React: GET /api/criterias?type=age
    Retourne la liste des param√®tres pour un type donn√©
    """
    try:
        type_name = request.args.get('type')
        if not type_name:
            return jsonify({ 'success': False, 'error': 'type query param requis' }), 400

        from .core.criteria_service import criteria_service
        criterias = criteria_service.get_criteria_by_type(type_name)

        return jsonify({
            'success': True,
            'criterias': criterias
        })
    except Exception as e:
        return jsonify({ 'success': False, 'error': str(e) }), 500

@app.route('/api/criterias/types', methods=['GET'])
def get_criteria_types():
    """
    R√©cup√®re tous les types de crit√®res disponibles
    SYST√àME DYNAMIQUE
    """
    try:
        from .core.criteria_service import criteria_service
        
        criteria_types = criteria_service.get_criteria_types()
        
        return jsonify({
            'success': True,
            'criteria_types': criteria_types
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/criterias/by-type/<string:type_name>', methods=['GET'])
def get_criterias_by_type(type_name):
    """
    R√©cup√®re tous les crit√®res d'un type sp√©cifique
    SYST√àME DYNAMIQUE
    
    Args:
        type_name: Type de crit√®re (age, thematique, accessibilite, etc.)
    """
    try:
        from .core.criteria_service import criteria_service
        
        criterias = criteria_service.get_criteria_by_type(type_name)
        
        return jsonify({
            'success': True,
            'type': type_name,
            'criterias': criterias,
            'count': len(criterias)
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/criterias/all', methods=['GET'])
def get_all_criterias():
    """
    R√©cup√®re tous les crit√®res group√©s par type
    SYST√àME DYNAMIQUE - Parfait pour alimenter un formulaire de s√©lection
    """
    try:
        from .core.criteria_service import criteria_service
        
        # R√©cup√©rer tous les types
        criteria_types = criteria_service.get_criteria_types()
        
        # Pour chaque type, r√©cup√©rer ses crit√®res
        result = []
        for ctype in criteria_types:
            criterias = criteria_service.get_criteria_by_type(ctype['type'])
            result.append({
                'type': ctype['type'],
                'label': ctype['label'],
                'ordre': ctype['ordre'],
                'is_required': ctype['is_required'],
                'options': criterias
            })
        
        return jsonify({
            'success': True,
            'criteria_groups': result,
            'total_types': len(result),
            'total_criterias': sum(len(g['options']) for g in result)
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/debug/pregenerations/<int:oeuvre_id>', methods=['GET'])
def debug_pregenerations(oeuvre_id):
    """Debug: v√©rifier les pr√©g√©n√©rations en BDD avec N crit√®res DYNAMIQUES"""
    try:
        from .core.db_postgres import _connect_postgres
        
        conn = _connect_postgres()
        cur = conn.cursor()
        
        # Requ√™te avec criterias en JSONB
        cur.execute("""
            SELECT 
                p.pregeneration_id,
                p.criteria_combination,
                LENGTH(p.pregeneration_text) as longueur, 
                LEFT(p.pregeneration_text, 200) as debut,
                ARRAY_AGG(
                    JSON_BUILD_OBJECT(
                        'type', c.type_name,
                        'name', c.name,
                        'label', c.label
                    )
                ) as criterias_detail
            FROM pregenerations p
            LEFT JOIN pregeneration_criterias pc ON p.pregeneration_id = pc.pregeneration_id
            LEFT JOIN criterias c ON pc.criteria_id = c.criteria_id
            WHERE p.oeuvre_id = %s 
            GROUP BY p.pregeneration_id
            ORDER BY p.pregeneration_id
        """, (oeuvre_id,))
        
        rows = cur.fetchall()
        
        pregenerations = []
        for row in rows:
            pregenerations.append({
                'pregeneration_id': row['pregeneration_id'],
                'criteria_combination': row['criteria_combination'],
                'criterias_detail': row['criterias_detail'],
                'longueur': row['longueur'],
                'debut': row['debut']
            })
        
        cur.close()
        conn.close()
        
        return jsonify({
            'success': True,
            'oeuvre_id': oeuvre_id,
            'count': len(pregenerations),
            'pregenerations': pregenerations
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ===== API OEUVRES =====

@app.route('/api/artworks', methods=['GET'])
def get_artworks_list():
    """R√©cup√®re toutes les ≈ìuvres"""
    try:
        artworks = get_all_artworks()
        return jsonify({
            'success': True,
            'count': len(artworks),
            'artworks': artworks
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/artworks/<int:artwork_id>', methods=['GET'])
def get_artwork_details(artwork_id):
    """R√©cup√®re d√©tails d'une ≈ìuvre"""
    try:
        artwork = get_artwork(artwork_id)
        if not artwork:
            return jsonify({
                'success': False,
                'error': '≈íuvre non trouv√©e'
            }), 404
        
        # Ajouter sections et anecdotes
        sections = get_artwork_sections(artwork_id)
        anecdotes = get_artwork_anecdotes(artwork_id)
        
        return jsonify({
            'success': True,
            'artwork': artwork,
            'sections': sections,
            'anecdotes': anecdotes
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/artworks/search', methods=['GET'])
def search_artworks_api():
    """Recherche d'≈ìuvres"""
    query = request.args.get('q', '').strip()
    if not query:
        return jsonify({
            'success': False,
            'error': 'Param√®tre de recherche manquant'
        }), 400
    
    try:
        results = search_artworks(query)
        return jsonify({
            'success': True,
            'query': query,
            'count': len(results),
            'results': results
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/artworks', methods=['POST'])
def create_artwork():
    """Cr√©e une nouvelle ≈ìuvre"""
    try:
        data = request.get_json()
        
        # Validation
        if not data.get('title') or not data.get('artist'):
            return jsonify({
                'success': False,
                'error': 'Titre et artiste requis'
            }), 400
        
        # Cr√©er ≈ìuvre
        oeuvre_id = add_artwork(
            title=data['title'],
            artist=data['artist'],
            description=data.get('description'),
            date_oeuvre=data.get('date_oeuvre'),
            materiaux_technique=data.get('materiaux_technique'),
            dimensions=data.get('dimensions'),
            image_link=data.get('image_link'),
            pdf_link=data.get('pdf_link'),
            room=data.get('room')
        )
        
        return jsonify({
            'success': True,
            'oeuvre_id': oeuvre_id
        }), 201
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ===== API PREGENERATIONS =====

@app.route('/api/pregenerations/stats', methods=['GET'])
def pregenerations_stats():
    """Statistiques pr√©g√©n√©rations"""
    try:
        stats = get_pregeneration_stats()
        return jsonify({
            'success': True,
            'stats': stats
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/pregenerations/<int:oeuvre_id>', methods=['GET'])
def get_artwork_pregen(oeuvre_id):
    """R√©cup√®re pr√©g√©n√©rations d'une ≈ìuvre"""
    try:
        pregenerations = get_artwork_pregenerations(oeuvre_id)
        return jsonify({
            'success': True,
            'oeuvre_id': oeuvre_id,
            'count': len(pregenerations),
            'pregenerations': pregenerations
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/pregenerations', methods=['POST'])
def create_pregeneration():
    """Cr√©e une pr√©g√©n√©ration avec N crit√®res DYNAMIQUES"""
    try:
        from .core.criteria_service import criteria_service
        
        data = request.get_json()
        
        # Validation des champs requis - FORMAT DYNAMIQUE avec dict
        required = ['oeuvre_id', 'criteria', 'pregeneration_text']
        for field in required:
            if field not in data:
                return jsonify({
                    'success': False,
                    'error': f'Champ requis: {field}'
                }), 400
        
        # criteria doit √™tre un dict, ex: {"age": 1, "thematique": 4, "style_texte": 7}
        criteria_dict = data['criteria']
        
        if not isinstance(criteria_dict, dict):
            return jsonify({
                'success': False,
                'error': 'Le champ "criteria" doit √™tre un objet JSON {type: id}'
            }), 400
        
        # Valider que tous les crit√®res obligatoires sont pr√©sents
        is_valid, missing = criteria_service.validate_required_criteria(criteria_dict)
        if not is_valid:
            return jsonify({
                'success': False,
                'error': f'Crit√®res obligatoires manquants: {", ".join(missing)}'
            }), 400
        
        # Valider que les crit√®res existent et sont actifs
        if not criteria_service.validate_criteria_combination(criteria_dict):
            return jsonify({
                'success': False,
                'error': 'Combinaison de crit√®res invalide ou crit√®res inactifs'
            }), 400
        
        # Cr√©er
        pregeneration_id = add_pregeneration(
            oeuvre_id=data['oeuvre_id'],
            criteria_dict=criteria_dict,
            pregeneration_text=data['pregeneration_text'],
            voice_link=data.get('voice_link')
        )
        
        return jsonify({
            'success': True,
            'pregeneration_id': pregeneration_id
        }), 201
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ===== API TRAITEMENT PDF =====

@app.route('/api/pdf/extract-metadata', methods=['POST'])
def extract_pdf_metadata():
    """
    Extrait les m√©tadonn√©es d'un PDF (titre, artiste, description, etc.)
    SANS cr√©er de chunks (traitement rapide 3-5 secondes)
    """
    print("üîç D√âBUT extraction m√©tadonn√©es PDF")
    try:
        # V√©rifier le fichier
        if 'file' not in request.files:
            print("‚ùå Aucun fichier dans la requ√™te")
            return jsonify({
                'success': False,
                'error': 'Aucun fichier fourni'
            }), 400
        
        file = request.files['file']
        print(f"üìÑ Fichier re√ßu: {file.filename}")
        
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            print(f"‚ùå Fichier non-PDF: {file.filename}")
            return jsonify({
                'success': False,
                'error': 'Fichier PDF requis'
            }), 400
        
        # Chemin du PDF (doit √™tre dans /app/uploads/pdfs/)
        pdf_path = request.form.get('pdf_path')
        print(f"üìÇ Chemin PDF re√ßu: {pdf_path}")
        
        if not pdf_path:
            print("‚ùå Aucun chemin PDF fourni")
            return jsonify({
                'success': False,
                'error': 'Chemin PDF requis (pdf_path)'
            }), 400
        
        # Construire le chemin complet
        full_path = f"/app{pdf_path}"
        print(f"üìÇ Chemin complet: {full_path}")
        
        # V√©rifier que le fichier existe
        if not os.path.exists(full_path):
            print(f"‚ùå Fichier non trouv√©: {full_path}")
            return jsonify({
                'success': False,
                'error': f'Fichier non trouv√©: {full_path}'
            }), 404
        
        print(f"‚úÖ Fichier trouv√©, d√©but extraction...")
        
        # Extraire les m√©tadonn√©es avec le processeur
        processor = ModelCompliantPDFProcessor()
        
        # Extraire le texte
        text = processor.extract_text_from_pdf(full_path)
        print(f"üìñ Texte extrait: {len(text)} caract√®res")
        if not text:
            print("‚ùå Texte vide")
            return jsonify({
                'success': False,
                'error': 'Impossible d\'extraire le texte du PDF'
            }), 400
        
        # Extraire les champs
        metadata = {}
        for field in processor.patterns.keys():
            value = processor.extract_field(text, field)
            if value:
                metadata[field] = value
                print(f"  ‚úì {field}: {value[:50]}...")
        
        print(f"‚úÖ M√©tadonn√©es extraites: {len(metadata)} champs")
        
        # Extraire les anecdotes
        anecdotes = processor.extract_anecdotes(text)
        print(f"üìù Anecdotes: {len(anecdotes)}")
        
        # Retourner les m√©tadonn√©es
        result = {
            'success': True,
            'metadata': {
                'title': metadata.get('titre', ''),
                'artist': metadata.get('artiste', ''),
                'lieu_naissance': metadata.get('lieu_naissance', ''),
                'date_oeuvre': metadata.get('date_oeuvre', ''),
                'materiaux': metadata.get('materiaux', ''),
                'mouvement': metadata.get('mouvement', ''),
                'provenance': metadata.get('provenance', ''),
                'contexte': metadata.get('contexte', ''),
                'description': metadata.get('description', ''),
                'analyse': metadata.get('analyse', ''),
                'iconographie': metadata.get('iconographie', ''),
                'reception': metadata.get('reception', ''),
                'parcours': metadata.get('parcours', ''),
                'anecdotes': anecdotes
            }
        }
        print(f"üéâ Retour: title='{result['metadata']['title']}', artist='{result['metadata']['artist']}'")
        return jsonify(result)
        
    except Exception as e:
        import traceback
        print(f"‚ùå Erreur extraction m√©tadonn√©es: {e}")
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/pdf/process-full', methods=['POST'])
def process_pdf_full():
    """
    Traitement complet d'un PDF : m√©tadonn√©es + chunks + embeddings
    (Plus long : 30 sec - 2 min)
    √Ä utiliser pour le traitement batch ou background
    """
    try:
        data = request.get_json()
        
        # R√©cup√©rer le chemin du PDF
        pdf_path = data.get('pdf_path')
        oeuvre_id = data.get('oeuvre_id')
        
        if not pdf_path:
            return jsonify({
                'success': False,
                'error': 'Chemin PDF requis (pdf_path)'
            }), 400
        
        # Construire le chemin complet
        full_path = f"/app{pdf_path}"
        
        # TODO: Impl√©menter le traitement complet
        # - Extraction m√©tadonn√©es (d√©j√† fait ci-dessus)
        # - D√©coupage en chunks
        # - G√©n√©ration embeddings
        # - Stockage dans la base
        
        return jsonify({
            'success': True,
            'message': 'Traitement complet √† impl√©menter (Phase 2)',
            'oeuvre_id': oeuvre_id
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ===== API PR√âG√âN√âRATION AUTOMATIQUE =====

# @app.route('/api/pregenerate-artwork/<int:oeuvre_id>', methods=['POST'])
# def pregenerate_single_artwork(oeuvre_id):
#     """Lance la pr√©g√©n√©ration COMPL√àTE avec Ollama pour une ≈ìuvre
#     Flux: Chunks ‚Üí Embeddings ‚Üí FAISS ‚Üí RAG ‚Üí Ollama ‚Üí 36 narrations uniques"""
#     try:
#         from .core.ollama_pregeneration_complete import get_ollama_pregeneration_system
        
#         # Options
#         data = request.get_json() or {}
#         force_regenerate = data.get('force_regenerate', False)
#         skip_rag_setup = data.get('skip_rag_setup', False)
        
#         # Lancer la pr√©g√©n√©ration COMPL√àTE
#         system = get_ollama_pregeneration_system()
#         result = system.pregenerate_artwork(
#             oeuvre_id=oeuvre_id,
#             force_regenerate=force_regenerate,
#             skip_rag_setup=skip_rag_setup
#         )
        
#         if result.get('success'):
#             stats = result.get('stats', {})
#             return jsonify({
#                 'success': True,
#                 'oeuvre_id': oeuvre_id,
#                 'title': result.get('title'),
#                 'stats': stats,
#                 'duration': result.get('duration'),
#                 'message': f"{stats.get('generated', 0)} narrations g√©n√©r√©es avec Ollama"
#             })
#         else:
#             return jsonify({
#                 'success': False,
#                 'error': result.get('error')
#             }), 500
        
#     except Exception as e:
#         import traceback
#         print(f"‚ùå Erreur pr√©g√©n√©ration Ollama: {e}")
#         print(traceback.format_exc())
#         return jsonify({
#             'success': False,
#             'error': str(e)
#         }), 500

@app.route('/api/pregenerate-artwork/<int:oeuvre_id>', methods=['POST'])
def pregenerate_single_artwork(oeuvre_id):
    try:
        data = request.get_json() or {}
        
        system = OllamaMediationSystem()
        all_criteres = get_criteres()
        result = system.pregenerate_artwork(
            oeuvre_id=oeuvre_id,
            artwork=get_artwork(oeuvre_id),
            combinaisons=system.generate_combinaisons(all_criteres),
            model="ministral-3:3b",
            force_regenerate=data.get('force_regenerate', False)
        )
        
        if result.get('success'):
            stats = result.get('stats', {})
            return jsonify({
                'success': True,
                'oeuvre_id': oeuvre_id,
                'title': result.get('title'),
                'stats': stats,
                'duration': result.get('duration'),
                'message': f"{stats.get('generated', 0)} narrations g√©n√©r√©es avec Ollama"
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error')
            }), 500
    except Exception as e:
        import traceback
        print(f"‚ùå Erreur pr√©g√©n√©ration Ollama: {e}")
        print(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500



@app.route('/api/pregenerate-all', methods=['POST'])
def pregenerate_all_artworks():
    """Lance la pr√©g√©n√©ration COMPL√àTE Ollama pour TOUTES les ≈ìuvres
    Utilise le m√™me syst√®me que pregenerate_single_artwork mais pour toutes les ≈ìuvres"""
    try:
        import time
        start_time = time.time()
        
        data = request.get_json() or {}
        force_regenerate = data.get('force_regenerate', False)
        
        # R√©cup√©rer toutes les ≈ìuvres
        conn = _connect_postgres()
        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        cur.execute("SELECT oeuvre_id FROM oeuvres ORDER BY oeuvre_id")
        oeuvres = cur.fetchall()
        cur.close()
        conn.close()
        
        if not oeuvres:
            return jsonify({
                'success': False,
                'error': 'Aucune ≈ìuvre trouv√©e dans la base de donn√©es'
            }), 404
        
        print(f"\n{'='*80}")
        print(f"üé® PR√âG√âN√âRATION GLOBALE - {len(oeuvres)} ≈íUVRES")
        print(f"{'='*80}")
        
        # Initialiser le syst√®me Ollama une seule fois
        system = OllamaMediationSystem()
        all_criteres = get_criteres()
        combinaisons = system.generate_combinaisons(all_criteres)
        
        print(f"üìã {len(combinaisons)} combinaisons de crit√®res √† g√©n√©rer par ≈ìuvre")
        
        total_stats = {
            'total_oeuvres': len(oeuvres),
            'total_generated': 0,
            'total_skipped': 0,
            'total_errors': 0,
            'oeuvres_processed': 0
        }
        
        # Traiter chaque ≈ìuvre
        for idx, oeuvre_row in enumerate(oeuvres):
            oeuvre_id = oeuvre_row['oeuvre_id']
            
            print(f"\n[{idx+1}/{len(oeuvres)}] Traitement ≈ìuvre ID {oeuvre_id}...")
            
            try:
                artwork = get_artwork(oeuvre_id)
                if not artwork:
                    print(f"   ‚ö†Ô∏è  ≈íuvre {oeuvre_id} non trouv√©e, skip")
                    total_stats['total_errors'] += 1
                    continue
                
                result = system.pregenerate_artwork(
                    oeuvre_id=oeuvre_id,
                    artwork=artwork,
                    combinaisons=combinaisons,
                    model="ministral-3:3b",
                    force_regenerate=force_regenerate
                )
                
                if result.get('success'):
                    stats = result.get('stats', {})
                    total_stats['total_generated'] += stats.get('generated', 0)
                    total_stats['total_skipped'] += stats.get('skipped', 0)
                    total_stats['oeuvres_processed'] += 1
                    print(f"   ‚úÖ {stats.get('generated', 0)} g√©n√©r√©es, {stats.get('skipped', 0)} skipp√©es")
                else:
                    total_stats['total_errors'] += 1
                    print(f"   ‚ùå Erreur: {result.get('error')}")
                    
            except Exception as e:
                total_stats['total_errors'] += 1
                print(f"   ‚ùå Exception: {e}")
                continue
        
        duration = time.time() - start_time
        duration_str = f"{int(duration // 60)}m {int(duration % 60)}s"
        
        print(f"\n{'='*80}")
        print(f"‚úÖ PR√âG√âN√âRATION GLOBALE TERMIN√âE")
        print(f"   - ≈íuvres trait√©es: {total_stats['oeuvres_processed']}/{total_stats['total_oeuvres']}")
        print(f"   - Narrations g√©n√©r√©es: {total_stats['total_generated']}")
        print(f"   - Narrations skipp√©es: {total_stats['total_skipped']}")
        print(f"   - Erreurs: {total_stats['total_errors']}")
        print(f"   - Dur√©e: {duration_str}")
        print(f"{'='*80}\n")
        
        return jsonify({
            'success': True,
            'message': 'Pr√©g√©n√©ration globale termin√©e avec Ollama',
            'stats': total_stats,
            'duration': duration_str
        })
        
    except Exception as e:
        import traceback
        print(f"‚ùå Erreur pr√©g√©n√©ration globale Ollama: {e}")
        print(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ===== API RAG + EMBEDDINGS + FAISS (REMOVED - LEGACY SYSTEM) =====
# Les endpoints suivants ont √©t√© supprim√©s car le syst√®me RAG/embeddings/FAISS
# n'est plus utilis√©. Le g√©n√©rateur OLLAMA utilise directement les m√©tadonn√©es DB.
# 
# Endpoints supprim√©s :
# - POST /api/rag/embeddings/create/<oeuvre_id>
# - POST /api/rag/faiss/build/<oeuvre_id>
# - POST /api/rag/faiss/build-global
# - POST /api/rag/search


# ===== API PR√âG√âN√âRATION AVEC LLM =====

@app.route('/api/llm/pregenerate-artwork/<int:oeuvre_id>', methods=['POST'])
def llm_pregenerate_artwork_api(oeuvre_id):
    """G√©n√®re les 36 narrations avec LLM pour une ≈ìuvre"""
    try:
        data = request.get_json() or {}
        force_regenerate = data.get('force_regenerate', False)
        llm_provider = data.get('llm_provider', 'groq')  # ollama, groq, openai
        
        from .core.llm_pregeneration import get_pregeneration_system
        
        system = get_pregeneration_system(llm_provider=llm_provider)
        result = system.pregenerate_artwork(
            oeuvre_id=oeuvre_id,
            force_regenerate=force_regenerate
        )
        
        return jsonify({
            'success': True,
            'message': f"{result.get('generated', 0)} narrations g√©n√©r√©es avec {llm_provider.upper()}",
            **result
        })
        
    except Exception as e:
        import traceback
        print(f"‚ùå Erreur pr√©g√©n√©ration LLM: {e}")
        print(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/llm/pregenerate-all', methods=['POST'])
def llm_pregenerate_all_api():
    """G√©n√®re les narrations LLM pour toutes les ≈ìuvres"""
    try:
        data = request.get_json() or {}
        force_regenerate = data.get('force_regenerate', False)
        llm_provider = data.get('llm_provider', 'groq')
        
        from .core.llm_pregeneration import get_pregeneration_system
        
        system = get_pregeneration_system(llm_provider=llm_provider)
        result = system.pregenerate_all_artworks(force_regenerate=force_regenerate)
        
        return jsonify({
            'success': True,
            **result
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ===== API CHUNKS & EMBEDDINGS (REMOVED - LEGACY SYSTEM) =====
# Endpoints supprim√©s :
# - POST /api/chunks/create/<oeuvre_id>
# - GET /api/chunks/<oeuvre_id>


# ===== API PARCOURS INTELLIGENT =====

@app.route('/api/parcours/generate', methods=['POST'])
def generate_intelligent_parcours():
    """
    G√©n√®re un parcours intelligent optimis√© bas√© sur une dur√©e cible
    AVEC g√©n√©ration automatique des fichiers audio TTS
    SYST√àME VRAIMENT DYNAMIQUE - Accepte N crit√®res variables
    
    Body JSON:
    {
        "criteria": {               # Dict flexible de N crit√®res
            "age": "adulte",        # Noms des crit√®res (seront r√©solus vers IDs)
            "thematique": "technique_picturale",
            "style_texte": "analyse"
            // Peut avoir 2, 5, ou N crit√®res !
        },
        "target_duration_minutes": 60,
        "variation_seed": 1234,
        "generate_audio": true
    }
    
    Returns: { success, parcours {...}, audio {...} }
    """
    
    try:
        # UTILISER V3
        from .parcours.intelligent_parcours_v3 import generate_parcours_v3
        from .core.criteria_service import criteria_service
        from .tts import get_piper_service
        import time
        
        print("üîµ [PARCOURS] Utilisation de generate_parcours_v3 (V3)")
        
        data = request.get_json()
        
        # Param√®tres obligatoires - FORMAT DYNAMIQUE
        criteria_names = data.get('criteria')  # Dict {type: name}
        
        if not criteria_names or not isinstance(criteria_names, dict):
            return jsonify({
                'success': False,
                'error': 'Param√®tre requis: "criteria" (objet {type: name})'
            }), 400
        
        # R√©soudre les noms vers IDs via criteria_service
        criteria_dict = {}  # {type: id}
        for type_name, criteria_name in criteria_names.items():
            criteria = criteria_service.get_criteria_by_name(type_name, criteria_name)
            if not criteria:
                return jsonify({
                    'success': False,
                    'error': f'Crit√®re invalide: {type_name}={criteria_name}'
                }), 400
            criteria_dict[type_name] = criteria['criteria_id']
        
        # Valider que tous les crit√®res obligatoires sont pr√©sents
        is_valid, missing = criteria_service.validate_required_criteria(criteria_dict)
        if not is_valid:
            return jsonify({
                'success': False,
                'error': f'Crit√®res obligatoires manquants: {", ".join(missing)}'
            }), 400
        
        # Param√®tres optionnels
        target_duration = data.get('target_duration_minutes', 60)
        variation_seed = data.get('variation_seed')
        generate_audio = data.get('generate_audio', True)
        
        # G√©n√©rer le parcours avec V3
        print(f"   üìê G√©n√©ration parcours V3: profile={criteria_dict}, duration={target_duration}min, seed={variation_seed}")
        parcours_json = generate_parcours_v3(
            profile=criteria_dict,
            target_duration_min=target_duration,
            seed=variation_seed
        )
        print(f"   ‚úÖ Parcours V3 g√©n√©r√©: {len(parcours_json.get('artworks', []))} ≈ìuvres")
        
        audio_result = {
            'generated': False,
            'count': 0,
            'paths': {}
        }
        
        # G√©n√©rer les audios si demand√©
        if generate_audio:
            print(f"\nüîµ [AUDIO DEBUG] generate_audio=True, d√©but g√©n√©ration audio")
            try:
                # Utiliser l'ID unique depuis les metadata (bas√© sur seed ou timestamp)
                parcours_id = parcours_json.get('metadata', {}).get('unique_parcours_id', variation_seed or int(time.time() * 1000))
                print(f"üîµ [AUDIO DEBUG] Parcours ID: {parcours_id}")
                
                # Pr√©parer les narrations pour TTS
                narrations = []
                for artwork in parcours_json.get('artworks', []):
                    narrations.append({
                        'oeuvre_id': artwork['oeuvre_id'],
                        'narration_text': artwork['narration']
                    })
                
                # G√©n√©rer les audios
                print(f"üîµ [AUDIO DEBUG] Pr√©paration de {len(narrations)} narrations pour TTS")
                for n in narrations:
                    print(f"   - Oeuvre {n['oeuvre_id']}: {len(n['narration_text'])} caract√®res")
                
                print(f"üîµ [AUDIO DEBUG] Appel get_piper_service()")
                piper = get_piper_service('fr_FR')
                print(f"üîµ [AUDIO DEBUG] Appel piper.generate_parcours_audio()")
                audio_results = piper.generate_parcours_audio(
                    parcours_id=parcours_id,
                    narrations=narrations,
                    language='fr_FR'
                )
                print(f"‚úÖ [AUDIO DEBUG] generate_parcours_audio() retourn√©: {len(audio_results)} r√©sultats")
                
                # Int√©grer les chemins audio ET dur√©es r√©elles dans les artworks
                for artwork in parcours_json.get('artworks', []):
                    oeuvre_id = artwork['oeuvre_id']
                    if oeuvre_id in audio_results:
                        audio_data = audio_results[oeuvre_id]
                        artwork['audio_path'] = audio_data['path']
                        # Mettre √† jour avec la dur√©e r√©elle du fichier audio
                        artwork['narration_duration'] = audio_data['duration_seconds']
                
                print(f"\nüìä CALCUL DES DUR√âES AVEC AUDIO R√âEL:")
                print(f"   Nombre d'≈ìuvres: {len(parcours_json['artworks'])}")
                
                # Recalculer UNIQUEMENT la dur√©e de narration avec les dur√©es r√©elles d'audio
                # Les dur√©es de marche et observation restent identiques
                total_narration_seconds = sum(artwork.get('narration_duration', 0) for artwork in parcours_json['artworks'])
                total_narration_minutes = total_narration_seconds / 60
                
                # R√©cup√©rer les valeurs existantes de marche et observation (inchang√©es)
                existing_walk_minutes = parcours_json['metadata']['duration_breakdown']['walking_minutes']
                existing_observation_minutes = parcours_json['metadata']['duration_breakdown']['observation_minutes']
                
                print(f"\n   üé§ Narration (audio r√©el):")
                print(f"      Total: {total_narration_seconds:.1f}s = {total_narration_minutes:.2f} min")
                for artwork in parcours_json['artworks']:
                    print(f"      - ≈íuvre {artwork['order']}: {artwork.get('narration_duration', 0):.1f}s")
                
                print(f"\n   üö∂ Marche (0.8 m/s):")
                print(f"      Total: {existing_walk_minutes:.2f} min")
                for artwork in parcours_json['artworks']:
                    walk = artwork.get('distance_to_next', 0)
                    if walk > 0:
                        print(f"      - ≈íuvre {artwork['order']} ‚Üí suivante: {walk:.2f} min")
                
                print(f"\n   üëÅÔ∏è Observation (2 min/≈ìuvre):")
                print(f"      Total: {existing_observation_minutes:.2f} min")
                
                # Mettre √† jour UNIQUEMENT narration_minutes et total_minutes
                parcours_json['metadata']['duration_breakdown']['narration_minutes'] = total_narration_minutes
                parcours_json['metadata']['duration_breakdown']['total_minutes'] = (
                    total_narration_minutes + existing_walk_minutes + existing_observation_minutes
                )
                
                print(f"\n   ‚è±Ô∏è DUR√âE TOTALE:")
                print(f"      {total_narration_minutes:.2f} min (narration)")
                print(f"    + {existing_walk_minutes:.2f} min (marche)")
                print(f"    + {existing_observation_minutes:.2f} min (observation)")
                print(f"    = {parcours_json['metadata']['duration_breakdown']['total_minutes']:.2f} min TOTAL")
                print(f"    = {parcours_json['metadata']['duration_breakdown']['total_minutes']/60:.1f}h\n")
                
                # Mettre √† jour aussi le champ racine estimated_duration_min (alias pour compatibilit√©)
                parcours_json['estimated_duration_min'] = parcours_json['metadata']['duration_breakdown']['total_minutes']
                
                audio_result = {
                    'generated': True,
                    'count': len(audio_results),
                    'paths': {oeuvre_id: data['path'] for oeuvre_id, data in audio_results.items()},
                    'durations': {oeuvre_id: data['duration_seconds'] for oeuvre_id, data in audio_results.items()}
                }
                
            except Exception as audio_error:
                # Si erreur audio, on continue quand m√™me avec le parcours
                print(f"‚ö†Ô∏è Erreur g√©n√©ration audio: {audio_error}")
                audio_result['error'] = str(audio_error)
        
        return jsonify({
            'success': True,
            'parcours': parcours_json,
            'audio': audio_result
        })
        
    except ValueError as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 404
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/parcours/map', methods=['POST'])
def get_parcours_map():
    """
    R√©cup√®re les donn√©es du plan pour visualiser un parcours
    
    Body JSON:
    {
        "artworks": [{oeuvre_id, order}, ...]
    }
    
    Returns:
    {
        "success": true,
        "map_data": {
            "rooms": [{room_id, name, polygon_points: [{x, y}]}],
            "artworks": [{oeuvre_id, title, x, y, room, order}]
        }
    }
    """
    
    try:
        from .core.db_postgres import _connect_postgres
        
        data = request.get_json()
        artworks_input = data.get('artworks', [])
        
        if not artworks_input:
            return jsonify({
                'success': False,
                'error': 'No artworks provided'
            }), 400
        
        conn = _connect_postgres()
        cur = conn.cursor()
        
        # R√©cup√©rer les IDs des ≈ìuvres
        oeuvre_ids = [a['oeuvre_id'] for a in artworks_input]
        order_map = {a['oeuvre_id']: a['order'] for a in artworks_input}
        
        # R√©cup√©rer les positions des ≈ìuvres (centre calcul√©)
        # Les ARTWORK ont 4 points (rectangle), on calcule le centre
        cur.execute("""
            SELECT 
                o.oeuvre_id,
                o.title,
                o.artist,
                o.room,
                AVG(p.x) as center_x,
                AVG(p.y) as center_y,
                COUNT(p.point_id) as point_count
            FROM oeuvres o
            LEFT JOIN entities e ON e.oeuvre_id = o.oeuvre_id AND e.entity_type = 'ARTWORK'
            LEFT JOIN points p ON p.entity_id = e.entity_id
            WHERE o.oeuvre_id = ANY(%s)
            GROUP BY o.oeuvre_id, o.title, o.artist, o.room
        """, (oeuvre_ids,))
        
        artworks_data = []
        artworks_without_position = []
        
        for row in cur.fetchall():
            if row['center_x'] is not None and row['center_y'] is not None:
                artworks_data.append({
                    'oeuvre_id': row['oeuvre_id'],
                    'title': row['title'],
                    'artist': row['artist'],
                    'room': row['room'],
                    'x': float(row['center_x']),
                    'y': float(row['center_y']),
                    'order': order_map.get(row['oeuvre_id'], 0)
                })
            else:
                artworks_without_position.append(row)
        
        # Si des ≈ìuvres n'ont pas de position, les placer au centre de leur salle (entity ROOM)
        if artworks_without_position:
            for artwork in artworks_without_position:
                # Trouver la salle ROOM entity qui correspond
                cur.execute("""
                    SELECT AVG(p.x) as center_x, AVG(p.y) as center_y
                    FROM entities e
                    JOIN points p ON p.entity_id = e.entity_id
                    WHERE e.entity_type = 'ROOM'
                    LIMIT 1
                """)
                
                center = cur.fetchone()
                if center and center['center_x']:
                    artworks_data.append({
                        'oeuvre_id': artwork['oeuvre_id'],
                        'title': artwork['title'],
                        'artist': artwork['artist'],
                        'room': artwork['room'],
                        'x': float(center['center_x']),
                        'y': float(center['center_y']),
                        'order': order_map.get(artwork['oeuvre_id'], 0)
                    })
                else:
                    # Fallback: position arbitraire
                    artworks_data.append({
                        'oeuvre_id': artwork['oeuvre_id'],
                        'title': artwork['title'],
                        'artist': artwork['artist'],
                        'room': artwork['room'],
                        'x': 100.0 + (artwork['oeuvre_id'] * 50),
                        'y': 100.0,
                        'order': order_map.get(artwork['oeuvre_id'], 0)
                    })
        
        # R√©cup√©rer TOUTES les salles du plan (pour avoir le contexte complet)
        cur.execute("""
            SELECT DISTINCT
                e.entity_id as room_id,
                e.name as room_name,
                p.x,
                p.y,
                p.ordre
            FROM entities e
            JOIN points p ON e.entity_id = p.entity_id
            WHERE e.entity_type = 'ROOM'
            ORDER BY e.entity_id, p.ordre
        """)
        
        rooms_dict = {}
        for row in cur.fetchall():
            room_id = row['room_id']
            if room_id not in rooms_dict:
                rooms_dict[room_id] = {
                    'room_id': room_id,
                    'name': row['room_name'] or f"Salle {room_id}",
                    'polygon_points': []
                }
            rooms_dict[room_id]['polygon_points'].append({
                'x': float(row['x']),
                'y': float(row['y'])
            })
        
        cur.close()
        conn.close()
        
        return jsonify({
            'success': True,
            'map_data': {
                'rooms': list(rooms_dict.values()),
                'artworks': sorted(artworks_data, key=lambda a: a['order'])
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/parcours/preview', methods=['GET'])
def preview_parcours_options():
    """
    Affiche les options disponibles pour g√©n√©rer un parcours
    
    Returns:
    {
        "success": true,
        "options": {
            "age_cible": ["enfant", "ado", "adulte", "senior"],
            "thematique": ["technique_picturale", "biographie", "historique"],
            "style_texte": ["analyse", "decouverte", "anecdote"]
        },
        "stats": {
            "total_artworks_with_narrations": 5,
            "artworks_per_profile": {...}
        }
    }
    """
    
    try:
        from .core.db_postgres import _connect_postgres
        
        conn = _connect_postgres()
        cur = conn.cursor()
        
        # Compter les ≈ìuvres par profil
        cur.execute("""
            SELECT 
                age_cible,
                thematique,
                style_texte,
                COUNT(DISTINCT oeuvre_id) as count
            FROM pregenerations
            GROUP BY age_cible, thematique, style_texte
            ORDER BY age_cible, thematique, style_texte
        """)
        
        rows = cur.fetchall()
        
        stats_per_profile = {}
        for row in rows:
            key = f"{row['age_cible']}_{row['thematique']}_{row['style_texte']}"
            stats_per_profile[key] = row['count']
        
        cur.close()
        conn.close()
        
        return jsonify({
            'success': True,
            'options': {
                'age_cible': ['enfant', 'ado', 'adulte', 'senior'],
                'thematique': ['technique_picturale', 'biographie', 'historique'],
                'style_texte': ['analyse', 'decouverte', 'anecdote']
            },
            'stats': {
                'artworks_per_profile': stats_per_profile,
                'total_profiles': len(stats_per_profile)
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/parcours', methods=['GET'])
def list_parcours():
    """
    Liste tous les parcours g√©n√©r√©s
    
    Returns:
    [
        {
            "group_id": "uuid",
            "segment_count": 5,
            "criteria": {"age": "Adulte", "thematique": "Technique picturale"},
            "created_at": "2024-01-15T10:30:00"
        },
        ...
    ]
    """
    try:
        from .core.db_postgres import _connect_postgres
        
        conn = _connect_postgres()
        cur = conn.cursor()
        
        # R√©cup√©rer la liste des parcours avec leurs infos
        cur.execute("""
            SELECT 
                group_id,
                COUNT(*) as segment_count,
                criteria_combination,
                MIN(created_at) as created_at
            FROM parcours_segments
            WHERE group_id IS NOT NULL
            GROUP BY group_id, criteria_combination
            ORDER BY created_at DESC
        """)
        
        rows = cur.fetchall()
        parcours_list = []
        
        for row in rows:
            criteria_dict = row[2] if row[2] else {}
            parcours_list.append({
                'group_id': row[0],
                'segment_count': row[1],
                'criteria': criteria_dict,
                'created_at': row[3].isoformat() if row[3] else None
            })
        
        cur.close()
        conn.close()
        
        return jsonify(parcours_list), 200
        
    except Exception as e:
        print(f"‚ùå Erreur liste parcours: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/parcours/<group_id>', methods=['GET'])
def get_parcours_details(group_id):
    """
    R√©cup√®re les d√©tails d'un parcours sp√©cifique
    
    Returns:
    {
        "group_id": "uuid",
        "segments": [
            {
                "id": 1,
                "segment_order": 1,
                "segment_type": "artwork",
                "guide_text": "...",
                "duration_minutes": 5,
                "oeuvre_info": {...}
            },
            ...
        ],
        "criteria": {"age": "Adulte", "thematique": "Technique picturale"}
    }
    """
    try:
        from .core.db_postgres import _connect_postgres
        
        conn = _connect_postgres()
        cur = conn.cursor()
        
        # R√©cup√©rer tous les segments du parcours
        cur.execute("""
            SELECT 
                id, segment_order, segment_type, guide_text,
                total_duration_minutes, oeuvre_info, criteria_combination
            FROM parcours_segments
            WHERE group_id = %s
            ORDER BY segment_order
        """, (group_id,))
        
        rows = cur.fetchall()
        
        if not rows:
            cur.close()
            conn.close()
            return jsonify({'error': 'Parcours not found'}), 404
        
        segments = []
        criteria = rows[0][6] if rows[0][6] else {}
        
        for row in rows:
            segments.append({
                'id': row[0],
                'segment_order': row[1],
                'segment_type': row[2],
                'guide_text': row[3],
                'duration_minutes': row[4] or 5,
                'oeuvre_info': row[5] or {}
            })
        
        cur.close()
        conn.close()
        
        return jsonify({
            'group_id': group_id,
            'segments': segments,
            'criteria': criteria
        }), 200
        
    except Exception as e:
        print(f"‚ùå Erreur d√©tails parcours: {e}")
        return jsonify({'error': str(e)}), 500


# ===== ADMIN ROUTES =====

@app.route('/api/admin/seed-narrations', methods=['POST'])
@app.route('/api/admin/seed-narrations/<int:oeuvre_id>', methods=['POST'])
def admin_seed_narrations(oeuvre_id=None):
    """
    Seed narrations avec script Python intelligent
    
    POST /api/admin/seed-narrations - Seed toutes les ≈ìuvres
    POST /api/admin/seed-narrations/<oeuvre_id> - Seed une ≈ìuvre sp√©cifique
    """
    try:
        import subprocess
        import json as json_module
        
        # Construire la commande Python
        script_path = Path(__file__).parent.parent / 'seed_narrations_dynamic.py'
        
        if not script_path.exists():
            return jsonify({
                'success': False,
                'error': f'Script seed introuvable: {script_path}'
            }), 404
        
        # Ex√©cuter le script Python
        result = subprocess.run(
            ['python', str(script_path)],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            return jsonify({
                'success': False,
                'error': f'Script seed failed: {result.stderr}'
            }), 500
        
        # Parser la sortie pour extraire les stats
        output = result.stdout
        inserted = 0
        skipped = 0
        
        # Chercher les lignes de r√©sultat
        for line in output.split('\n'):
            if 'nouvelles narrations ins√©r√©es' in line:
                try:
                    inserted = int(line.split('-')[1].strip().split()[0])
                except:
                    pass
            if 'combinaisons d√©j√† existantes' in line:
                try:
                    skipped = int(line.split('-')[1].strip().split()[0])
                except:
                    pass
        
        return jsonify({
            'success': True,
            'inserted': inserted,
            'skipped': skipped,
            'message': 'Seed termin√© avec succ√®s'
        }), 200
        
    except Exception as e:
        print(f"‚ùå Erreur seed narrations: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/admin/delete-all-narrations', methods=['DELETE'])
def admin_delete_all_narrations():
    """
    Supprime TOUTES les narrations de la base
    Action irr√©versible !
    """
    try:
        conn = _connect_postgres()
        cur = conn.cursor()
        
        # Compter avant suppression
        cur.execute("SELECT COUNT(*) as count FROM pregenerations")
        result = cur.fetchone()
        count = result['count'] if result else 0
        
        # Supprimer (CASCADE supprimera aussi pregeneration_criterias)
        cur.execute("TRUNCATE TABLE pregenerations CASCADE")
        conn.commit()
        
        cur.close()
        conn.close()
        
        return jsonify({
            'success': True,
            'deleted': count,
            'message': f'{count} narrations supprim√©es'
        }), 200
        
    except Exception as e:
        print(f"‚ùå Erreur suppression narrations: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


# ===== MUSEUM FLOOR PLAN =====

@app.route('/api/museum/floor-plan', methods=['GET'])
def get_floor_plan():
    """
    R√©cup√®re le plan du mus√©e (salles avec polygones) pour affichage
    
    Query params optionnels:
        - floor: int (filtrer par √©tage)
    
    Returns:
        {
            "success": true,
            "rooms": [
                {
                    "entity_id": 1,
                    "name": "Salle 1",
                    "floor": 0,
                    "polygon_points": [{x, y}, ...]
                },
                ...
            ]
        }
    """
    try:
        conn = _connect_postgres()
        cur = conn.cursor()
        
        # Filtrer par √©tage si sp√©cifi√©
        floor_filter = request.args.get('floor')
        
        # D'abord, cr√©er un mapping plan_id ‚Üí floor_num
        cur.execute("""
            SELECT plan_id, nom 
            FROM plans 
            ORDER BY plan_id
        """)
        plan_to_floor = {}
        for idx, row in enumerate(cur.fetchall()):
            plan_to_floor[row['plan_id']] = idx
        
        # R√©cup√©rer les salles avec leurs polygones et plan_id
        query = """
            SELECT 
                e.entity_id,
                e.name,
                e.plan_id,
                array_agg(p.x ORDER BY p.ordre) as xs,
                array_agg(p.y ORDER BY p.ordre) as ys
            FROM entities e
            LEFT JOIN points p ON e.entity_id = p.entity_id
            WHERE e.entity_type = 'ROOM'
            GROUP BY e.entity_id, e.name, e.plan_id
            ORDER BY e.entity_id
        """
        
        cur.execute(query)
        rows = cur.fetchall()
        
        rooms = []
        for row in rows:
            # Utiliser plan_id pour d√©terminer l'√©tage
            floor_num = plan_to_floor.get(row['plan_id'], 0)
            
            # Filtrer par √©tage si demand√©
            if floor_filter is not None and floor_num != int(floor_filter):
                continue
            
            # Construire polygone
            xs = row['xs'] or []
            ys = row['ys'] or []
            polygon_points = [{'x': x, 'y': y} for x, y in zip(xs, ys)]
            
            rooms.append({
                'entity_id': row['entity_id'],
                'name': row['name'],
                'floor': floor_num,
                'polygon_points': polygon_points
            })
        
        cur.close()
        conn.close()
        
        return jsonify({
            'success': True,
            'rooms': rooms
        }), 200
        
    except Exception as e:
        print(f"‚ùå Erreur r√©cup√©ration floor plan: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/admin/generate-narration-precise', methods=['POST'])
def admin_generate_narration_precise():
    """
    G√©n√®re UNE narration pr√©cise pour 1 ≈ìuvre + 1 profil sp√©cifique
    
    POST /api/admin/generate-narration-precise
    Body: {
      "oeuvre_id": 1,
      "criteria_combination": { "age": 1, "thematique": 5, "style_texte": 8 }
    }
    """
    try:
        import json as json_module
        
        data = request.get_json()
        oeuvre_id = data.get('oeuvre_id')
        criteria_combination = data.get('criteria_combination')
        
        if not oeuvre_id or not criteria_combination:
            return jsonify({
                'success': False,
                'error': 'oeuvre_id et criteria_combination requis'
            }), 400
        
        conn = _connect_postgres()
        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Charger l'≈ìuvre COMPL√àTE avec toutes m√©tadonn√©es
        cur.execute("""
            SELECT oeuvre_id, title, artist, description, date_oeuvre,
                   materiaux_technique, provenance, contexte_commande,
                   analyse_materielle_technique, iconographie_symbolique,
                   anecdotes, reception_circulation_posterite,
                   parcours_conservation_doc, room
            FROM oeuvres WHERE oeuvre_id = %s
        """, (oeuvre_id,))
        artwork = cur.fetchone()
        
        if not artwork:
            cur.close()
            conn.close()
            return jsonify({
                'success': False,
                'error': f'≈íuvre {oeuvre_id} non trouv√©e'
            }), 404
        
        # Charger les crit√®res d√©taill√©s (avec name, description, ai_indication)
        all_criteres = get_criteres()
        
        # Construire la combinaison enrichie avec les d√©tails des crit√®res
        combinaison_enrichie = {}
        for crit_type, crit_id in criteria_combination.items():
            criteres_type = all_criteres.get(crit_type, [])
            critere_detail = next((c for c in criteres_type if c['criteria_id'] == crit_id), None)
            if critere_detail:
                combinaison_enrichie[crit_type] = critere_detail
        
        if len(combinaison_enrichie) != len(criteria_combination):
            cur.close()
            conn.close()
            return jsonify({
                'success': False,
                'error': 'Crit√®res invalides dans la combinaison'
            }), 400
        
        # Initialiser le syst√®me Ollama
        ollama_system = OllamaMediationSystem()
        
        # G√©n√©rer la narration avec le syst√®me complet
        result = ollama_system.generate_mediation_for_one_work(
            artwork=dict(artwork),
            combinaison=combinaison_enrichie,
            duree_minutes=3
        )
        
        if not result['success']:
            cur.close()
            conn.close()
            return jsonify({
                'success': False,
                'error': result.get('error', 'Erreur g√©n√©ration Ollama')
            }), 500
        
        narration = result['text']
        
        # UPSERT dans la DB
        cur.execute("""
            INSERT INTO pregenerations (
                oeuvre_id, 
                criteria_combination, 
                pregeneration_text,
                created_at,
                updated_at
            ) VALUES (%s, %s, %s, NOW(), NOW())
            ON CONFLICT (oeuvre_id, criteria_combination) 
            DO UPDATE SET 
                pregeneration_text = EXCLUDED.pregeneration_text,
                updated_at = NOW()
            RETURNING pregeneration_id, created_at
        """, (
            oeuvre_id,
            json_module.dumps(criteria_combination),
            narration
        ))
        
        db_result = cur.fetchone()
        
        conn.commit()
        cur.close()
        conn.close()
        
        profile_str = ' / '.join([combinaison_enrichie[k]['name'] for k in combinaison_enrichie])
        print(f"‚úÖ Narration g√©n√©r√©e pr√©cise: oeuvre_id={oeuvre_id}, profil={profile_str}")
        
        return jsonify({
            'success': True,
            'pregeneration': {
                'pregeneration_id': db_result['pregeneration_id'],
                'oeuvre_id': oeuvre_id,
                'criteria_combination': criteria_combination,
                'pregeneration_text': narration,
                'created_at': str(db_result['created_at'])
            }
        }), 200
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration narration pr√©cise: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/admin/generate-narrations-by-profile', methods=['POST'])
def admin_generate_narrations_by_profile():
    """
    G√©n√®re les narrations pour 1 profil sp√©cifique dans TOUTES les ≈ìuvres
    
    POST /api/admin/generate-narrations-by-profile
    Body: {
      "criteria_combination": { "age": 1, "thematique": 5, "style_texte": 8 }
    }
    """
    try:
        import json as json_module
        
        data = request.get_json()
        criteria_combination = data.get('criteria_combination')
        
        if not criteria_combination:
            return jsonify({
                'success': False,
                'error': 'criteria_combination requis'
            }), 400
        
        conn = _connect_postgres()
        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Charger toutes les ≈ìuvres
        cur.execute("SELECT oeuvre_id, title, artist FROM oeuvres ORDER BY oeuvre_id")
        artworks = cur.fetchall()
        
        if not artworks:
            return jsonify({
                'success': False,
                'error': 'Aucune ≈ìuvre trouv√©e'
            }), 404
        
        inserted = 0
        skipped = 0
        errors = []
        
        profile_str = ' / '.join([f"{k}:{v}" for k, v in criteria_combination.items()])
        
        # Charger les crit√®res d√©taill√©s
        all_criteres = get_criteres()
        
        # Construire la combinaison enrichie
        combinaison_enrichie = {}
        for crit_type, crit_id in criteria_combination.items():
            criteres_type = all_criteres.get(crit_type, [])
            critere_detail = next((c for c in criteres_type if c['criteria_id'] == crit_id), None)
            if critere_detail:
                combinaison_enrichie[crit_type] = critere_detail
        
        if len(combinaison_enrichie) != len(criteria_combination):
            cur.close()
            conn.close()
            return jsonify({
                'success': False,
                'error': 'Crit√®res invalides dans la combinaison'
            }), 400
        
        # Initialiser le syst√®me Ollama
        ollama_system = OllamaMediationSystem()
        
        # G√©n√©rer pour chaque ≈ìuvre
        for artwork in artworks:
            try:
                # V√©rifier si d√©j√† exists
                cur.execute("""
                    SELECT pregeneration_id FROM pregenerations 
                    WHERE oeuvre_id = %s AND criteria_combination = %s
                """, (artwork['oeuvre_id'], json_module.dumps(criteria_combination)))
                
                if cur.fetchone():
                    skipped += 1
                    continue
                
                # Charger m√©tadonn√©es compl√®tes de l'≈ìuvre
                cur.execute("""
                    SELECT oeuvre_id, title, artist, description, date_oeuvre,
                           materiaux_technique, provenance, contexte_commande,
                           analyse_materielle_technique, iconographie_symbolique,
                           anecdotes, reception_circulation_posterite,
                           parcours_conservation_doc, room
                    FROM oeuvres WHERE oeuvre_id = %s
                """, (artwork['oeuvre_id'],))
                full_artwork = cur.fetchone()
                
                if not full_artwork:
                    errors.append(f"‚ö†Ô∏è  {artwork['title']}: M√©tadonn√©es non trouv√©es")
                    skipped += 1
                    continue
                
                # G√©n√©rer la narration avec le syst√®me complet
                result = ollama_system.generate_mediation_for_one_work(
                    artwork=dict(full_artwork),
                    combinaison=combinaison_enrichie,
                    duree_minutes=3
                )
                
                if not result['success']:
                    errors.append(f"‚ö†Ô∏è  {artwork['title']}: {result.get('error', 'Ollama g√©n√©ration √©chou√©e')}")
                    skipped += 1
                    continue
                
                narration = result['text']
                
                # Ins√©rer dans la DB
                cur.execute("""
                    INSERT INTO pregenerations (
                        oeuvre_id, 
                        criteria_combination, 
                        pregeneration_text,
                        created_at,
                        updated_at
                    ) VALUES (%s, %s, %s, NOW(), NOW())
                """, (
                    artwork['oeuvre_id'],
                    json_module.dumps(criteria_combination),
                    narration
                ))
                
                inserted += 1
                print(f"‚úÖ {artwork['title']}: narration g√©n√©r√©e")
                
            except Exception as e:
                errors.append(f"‚ùå {artwork['title']}: {str(e)}")
                skipped += 1
        
        conn.commit()
        cur.close()
        conn.close()
        
        message = f"{inserted} narrations g√©n√©r√©es, {skipped} skipp√©es"
        if errors:
            message += f"\n\nErreurs:\n" + '\n'.join(errors[:5])
        
        print(f"‚úÖ G√©n√©ration par profil {profile_str} compl√©t√©e: {message}")
        
        return jsonify({
            'success': True,
            'inserted': inserted,
            'skipped': skipped,
            'errors': errors,
            'message': message
        }), 200
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration par profil: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


# ===== API NETTOYAGE AUDIO =====

@app.route('/api/cleanup/audio', methods=['POST'])
def cleanup_audio_files():
    """
    Nettoie les fichiers audio des sessions expir√©es
    Appel√© manuellement ou p√©riodiquement par un cron job
    """
    try:
        from .core.cleanup_service import get_cleanup_service
        
        cleanup_service = get_cleanup_service()
        cleaned_count = cleanup_service.cleanup_all()
        
        return jsonify({
            'success': True,
            'cleaned': cleaned_count,
            'message': f'{cleaned_count} dossiers audio nettoy√©s'
        }), 200
        
    except Exception as e:
        print(f"‚ùå Erreur nettoyage audio: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/cleanup/status', methods=['GET'])
def cleanup_status():
    """Retourne les statistiques de nettoyage"""
    try:
        from .core.cleanup_service import get_cleanup_service
        import os
        from pathlib import Path
        
        audio_dir = Path("/app/uploads/audio")
        
        # Compter les dossiers audio existants
        parcours_count = 0
        if audio_dir.exists():
            parcours_count = len([d for d in audio_dir.iterdir() if d.is_dir() and d.name.startswith('parcours_')])
        
        return jsonify({
            'success': True,
            'active_parcours': parcours_count,
            'audio_directory': str(audio_dir)
        }), 200
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/admin/cleanup-all-sessions', methods=['POST'])
def cleanup_all_sessions():
    """
    Force le nettoyage de TOUTES les sessions actives et leurs donn√©es
    Supprime tous les tokens et tous les dossiers audio
    ATTENTION: Action irr√©versible r√©serv√©e aux admins
    """
    try:
        import os
        import shutil
        from pathlib import Path
        
        conn = _connect_postgres()
        cur = conn.cursor()
        
        # Compter les sessions √† supprimer
        cur.execute("SELECT COUNT(*) as count FROM qr_code WHERE parcours_id IS NOT NULL")
        result = cur.fetchone()
        session_count = result['count'] if result else 0
        
        # R√©cup√©rer tous les parcours_id avant suppression
        cur.execute("SELECT DISTINCT parcours_id FROM qr_code WHERE parcours_id IS NOT NULL")
        parcours_ids = [row['parcours_id'] for row in cur.fetchall()]
        
        # Supprimer toutes les sessions de la BDD
        cur.execute("DELETE FROM qr_code")
        conn.commit()
        
        # Supprimer tous les dossiers audio
        audio_dir = Path("/app/uploads/audio")
        deleted_folders = 0
        
        if audio_dir.exists():
            for audio_folder in audio_dir.iterdir():
                if audio_folder.is_dir() and audio_folder.name.startswith('parcours_'):
                    try:
                        shutil.rmtree(audio_folder)
                        deleted_folders += 1
                        print(f"üóëÔ∏è Supprim√©: {audio_folder.name}")
                    except Exception as e:
                        print(f"‚ùå Erreur suppression {audio_folder.name}: {e}")
        
        cur.close()
        conn.close()
        
        print(f"‚úÖ Nettoyage complet: {session_count} sessions et {deleted_folders} dossiers audio supprim√©s")
        
        return jsonify({
            'success': True,
            'deleted_sessions': session_count,
            'deleted_audio_folders': deleted_folders,
            'message': f'{session_count} sessions et {deleted_folders} dossiers audio supprim√©s'
        }), 200
        
    except Exception as e:
        print(f"‚ùå Erreur nettoyage complet: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ===== D√âMARRAGE =====

if __name__ == '__main__':
    print("üöÄ D√©marrage Museum Voice Backend")
    print("üìç Port: 5000")
    print("üóÑÔ∏è  Database: PostgreSQL")
    app.run(host='0.0.0.0', port=5000, debug=True)
